{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123420\n",
      "123420\n",
      "123420\n"
     ]
    }
   ],
   "source": [
    "with open(\"5_4_3reviews.txt\") as f:\n",
    "    reviews = f.read().split(\"\\n\")\n",
    "with open(\"5_4_3labels.txt\") as f:\n",
    "    labels = f.read().split(\"\\n\")\n",
    "    \n",
    "reviews_tokens = [review.split() for review in reviews]\n",
    "print(len(reviews_tokens))\n",
    "print(len(labels))\n",
    "print(len(reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiLabelBinarizer(classes=None, sparse_output=False)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "onehot_enc = MultiLabelBinarizer()\n",
    "onehot_enc.fit(reviews_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(reviews_tokens, labels, test_size=0.4, random_state=None)\n",
    "\n",
    "split_point = int(len(X_test)/2)\n",
    "X_valid, y_valid = X_test[split_point:], y_test[split_point:]\n",
    "X_test, y_test = X_test[:split_point], y_test[:split_point]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "vocab_len = len(onehot_enc.classes_)\n",
    "inputs_ = tf.placeholder(dtype=tf.float32, shape=[None, vocab_len], name=\"inputs\")\n",
    "targets_ = tf.placeholder(dtype=tf.float32, shape=[None, 2], name=\"targets\")\n",
    "\n",
    "h1 = tf.layers.dense(inputs_, 500, activation=tf.nn.relu)\n",
    "#h2 = tf.layers.dense(h1, 500, activation=tf.nn.relu)\n",
    "logits = tf.layers.dense(h1, 2, activation=None)\n",
    "output = tf.nn.sigmoid(logits)\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=targets_))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(0.001).minimize(loss)\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(targets_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def label2bool(labels):\n",
    "    return [[1,0] if label == \"positive\" else [0,1] for label in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(X, y, batch_size):\n",
    "    for batch_pos in range(0,len(X),batch_size):\n",
    "        yield X[batch_pos:batch_pos+batch_size], y[batch_pos:batch_pos+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \t Training loss: 0.692522764206\n",
      "Epoch: 0 \t Training loss: 0.580520272255\n",
      "Epoch: 0 \t Training loss: 0.511491715908\n",
      "Epoch: 0 \t Training loss: 0.475391715765\n",
      "Epoch: 0 \t Training loss: 0.436409413815\n",
      "Epoch: 0 \t Training loss: 0.438715726137\n",
      "Epoch: 0 \t Training loss: 0.428619384766\n",
      "Epoch: 0 \t Training loss: 0.420236736536\n",
      "Epoch: 0 \t Training loss: 0.452311128378\n",
      "Epoch: 0 \t Training loss: 0.416329741478\n",
      "Epoch: 0 \t Training loss: 0.433669120073\n",
      "Epoch: 0 \t Training loss: 0.444027274847\n",
      "Epoch: 0 \t Training loss: 0.389065802097\n",
      "Epoch: 0 \t Training loss: 0.393432706594\n",
      "Epoch: 0 \t Training loss: 0.399016797543\n",
      "Epoch: 0 \t Training loss: 0.378523856401\n",
      "Epoch: 0 \t Training loss: 0.418591231108\n",
      "Epoch: 0 \t Training loss: 0.376725673676\n",
      "Epoch: 0 \t Training loss: 0.371175140142\n",
      "Epoch: 0 \t Training loss: 0.367582261562\n",
      "Epoch: 0 \t Training loss: 0.368298172951\n",
      "Epoch: 0 \t Training loss: 0.356698483229\n",
      "Epoch: 0 \t Training loss: 0.390983968973\n",
      "Epoch: 0 \t Training loss: 0.375213772058\n",
      "Epoch: 0 \t Training loss: 0.33345541358\n",
      "Epoch: 0 \t Validation Accuracy: 0.882960617542\n",
      "Epoch: 1 \t Training loss: 0.356876790524\n",
      "Epoch: 1 \t Training loss: 0.345715820789\n",
      "Epoch: 1 \t Training loss: 0.353204190731\n",
      "Epoch: 1 \t Training loss: 0.358663260937\n",
      "Epoch: 1 \t Training loss: 0.335876375437\n",
      "Epoch: 1 \t Training loss: 0.345394015312\n",
      "Epoch: 1 \t Training loss: 0.354118108749\n",
      "Epoch: 1 \t Training loss: 0.339502364397\n",
      "Epoch: 1 \t Training loss: 0.332020550966\n",
      "Epoch: 1 \t Training loss: 0.333109885454\n",
      "Epoch: 1 \t Training loss: 0.324520260096\n",
      "Epoch: 1 \t Training loss: 0.368817657232\n",
      "Epoch: 1 \t Training loss: 0.333265393972\n",
      "Epoch: 1 \t Training loss: 0.332239478827\n",
      "Epoch: 1 \t Training loss: 0.339401394129\n",
      "Epoch: 1 \t Training loss: 0.322209686041\n",
      "Epoch: 1 \t Training loss: 0.362710624933\n",
      "Epoch: 1 \t Training loss: 0.332615613937\n",
      "Epoch: 1 \t Training loss: 0.33878198266\n",
      "Epoch: 1 \t Training loss: 0.343153804541\n",
      "Epoch: 1 \t Training loss: 0.334887415171\n",
      "Epoch: 1 \t Training loss: 0.321187019348\n",
      "Epoch: 1 \t Training loss: 0.364834159613\n",
      "Epoch: 1 \t Training loss: 0.34378027916\n",
      "Epoch: 1 \t Training loss: 0.294793188572\n",
      "Epoch: 1 \t Validation Accuracy: 0.882960617542\n",
      "Epoch: 2 \t Training loss: 0.329424798489\n",
      "Epoch: 2 \t Training loss: 0.320468395948\n",
      "Epoch: 2 \t Training loss: 0.328965038061\n",
      "Epoch: 2 \t Training loss: 0.336254268885\n",
      "Epoch: 2 \t Training loss: 0.314097046852\n",
      "Epoch: 2 \t Training loss: 0.320033073425\n",
      "Epoch: 2 \t Training loss: 0.332942575216\n",
      "Epoch: 2 \t Training loss: 0.318551421165\n",
      "Epoch: 2 \t Training loss: 0.312224715948\n",
      "Epoch: 2 \t Training loss: 0.313944846392\n",
      "Epoch: 2 \t Training loss: 0.305178642273\n",
      "Epoch: 2 \t Training loss: 0.344895601273\n",
      "Epoch: 2 \t Training loss: 0.312409549952\n",
      "Epoch: 2 \t Training loss: 0.310399889946\n",
      "Epoch: 2 \t Training loss: 0.320066779852\n",
      "Epoch: 2 \t Training loss: 0.299657225609\n",
      "Epoch: 2 \t Training loss: 0.338250398636\n",
      "Epoch: 2 \t Training loss: 0.311522781849\n",
      "Epoch: 2 \t Training loss: 0.319007009268\n",
      "Epoch: 2 \t Training loss: 0.321292728186\n",
      "Epoch: 2 \t Training loss: 0.313668072224\n",
      "Epoch: 2 \t Training loss: 0.303151726723\n",
      "Epoch: 2 \t Training loss: 0.344461679459\n",
      "Epoch: 2 \t Training loss: 0.323958456516\n",
      "Epoch: 2 \t Training loss: 0.27683904767\n",
      "Epoch: 2 \t Validation Accuracy: 0.882960617542\n",
      "Epoch: 3 \t Training loss: 0.313046664\n",
      "Epoch: 3 \t Training loss: 0.306363135576\n",
      "Epoch: 3 \t Training loss: 0.310710698366\n",
      "Epoch: 3 \t Training loss: 0.318577080965\n",
      "Epoch: 3 \t Training loss: 0.297511726618\n",
      "Epoch: 3 \t Training loss: 0.301897168159\n",
      "Epoch: 3 \t Training loss: 0.316253811121\n",
      "Epoch: 3 \t Training loss: 0.303329914808\n",
      "Epoch: 3 \t Training loss: 0.295613765717\n",
      "Epoch: 3 \t Training loss: 0.298032641411\n",
      "Epoch: 3 \t Training loss: 0.289417535067\n",
      "Epoch: 3 \t Training loss: 0.327532827854\n",
      "Epoch: 3 \t Training loss: 0.29739677906\n",
      "Epoch: 3 \t Training loss: 0.29533803463\n",
      "Epoch: 3 \t Training loss: 0.305261909962\n",
      "Epoch: 3 \t Training loss: 0.283973753452\n",
      "Epoch: 3 \t Training loss: 0.318887829781\n",
      "Epoch: 3 \t Training loss: 0.296795070171\n",
      "Epoch: 3 \t Training loss: 0.305195212364\n",
      "Epoch: 3 \t Training loss: 0.3062672019\n",
      "Epoch: 3 \t Training loss: 0.298312366009\n",
      "Epoch: 3 \t Training loss: 0.288611978292\n",
      "Epoch: 3 \t Training loss: 0.327611744404\n",
      "Epoch: 3 \t Training loss: 0.305870890617\n",
      "Epoch: 3 \t Training loss: 0.261889487505\n",
      "Epoch: 3 \t Validation Accuracy: 0.882960617542\n",
      "Epoch: 4 \t Training loss: 0.30007186532\n",
      "Epoch: 4 \t Training loss: 0.294893473387\n",
      "Epoch: 4 \t Training loss: 0.297030597925\n",
      "Epoch: 4 \t Training loss: 0.30507299304\n",
      "Epoch: 4 \t Training loss: 0.284523159266\n",
      "Epoch: 4 \t Training loss: 0.288394004107\n",
      "Epoch: 4 \t Training loss: 0.302554965019\n",
      "Epoch: 4 \t Training loss: 0.29046612978\n",
      "Epoch: 4 \t Training loss: 0.28242623806\n",
      "Epoch: 4 \t Training loss: 0.285890460014\n",
      "Epoch: 4 \t Training loss: 0.278320968151\n",
      "Epoch: 4 \t Training loss: 0.313587397337\n",
      "Epoch: 4 \t Training loss: 0.286194950342\n",
      "Epoch: 4 \t Training loss: 0.284102082253\n",
      "Epoch: 4 \t Training loss: 0.293574184179\n",
      "Epoch: 4 \t Training loss: 0.271861433983\n",
      "Epoch: 4 \t Training loss: 0.305259376764\n",
      "Epoch: 4 \t Training loss: 0.285844892263\n",
      "Epoch: 4 \t Training loss: 0.294239997864\n",
      "Epoch: 4 \t Training loss: 0.294611126184\n",
      "Epoch: 4 \t Training loss: 0.286930859089\n",
      "Epoch: 4 \t Training loss: 0.277117758989\n",
      "Epoch: 4 \t Training loss: 0.314029306173\n",
      "Epoch: 4 \t Training loss: 0.291816115379\n",
      "Epoch: 4 \t Training loss: 0.25172740221\n",
      "Epoch: 4 \t Validation Accuracy: 0.882960617542\n",
      "Epoch: 5 \t Training loss: 0.29027312994\n",
      "Epoch: 5 \t Training loss: 0.286112785339\n",
      "Epoch: 5 \t Training loss: 0.286868423223\n",
      "Epoch: 5 \t Training loss: 0.295421153307\n",
      "Epoch: 5 \t Training loss: 0.275098741055\n",
      "Epoch: 5 \t Training loss: 0.278591483831\n",
      "Epoch: 5 \t Training loss: 0.291830271482\n",
      "Epoch: 5 \t Training loss: 0.280438989401\n",
      "Epoch: 5 \t Training loss: 0.273038506508\n",
      "Epoch: 5 \t Training loss: 0.277097642422\n",
      "Epoch: 5 \t Training loss: 0.27049022913\n",
      "Epoch: 5 \t Training loss: 0.302728235722\n",
      "Epoch: 5 \t Training loss: 0.277969926596\n",
      "Epoch: 5 \t Training loss: 0.275431990623\n",
      "Epoch: 5 \t Training loss: 0.284146219492\n",
      "Epoch: 5 \t Training loss: 0.262932687998\n",
      "Epoch: 5 \t Training loss: 0.294593811035\n",
      "Epoch: 5 \t Training loss: 0.277492761612\n",
      "Epoch: 5 \t Training loss: 0.285704880953\n",
      "Epoch: 5 \t Training loss: 0.285191774368\n",
      "Epoch: 5 \t Training loss: 0.277882188559\n",
      "Epoch: 5 \t Training loss: 0.26778909564\n",
      "Epoch: 5 \t Training loss: 0.302768528461\n",
      "Epoch: 5 \t Training loss: 0.280709952116\n",
      "Epoch: 5 \t Training loss: 0.243462309241\n",
      "Epoch: 5 \t Validation Accuracy: 0.882960617542\n",
      "Epoch: 6 \t Training loss: 0.282398998737\n",
      "Epoch: 6 \t Training loss: 0.278464943171\n",
      "Epoch: 6 \t Training loss: 0.278854459524\n",
      "Epoch: 6 \t Training loss: 0.287499964237\n",
      "Epoch: 6 \t Training loss: 0.26716452837\n",
      "Epoch: 6 \t Training loss: 0.270930677652\n",
      "Epoch: 6 \t Training loss: 0.283274680376\n",
      "Epoch: 6 \t Training loss: 0.271997630596\n",
      "Epoch: 6 \t Training loss: 0.265173345804\n",
      "Epoch: 6 \t Training loss: 0.269531995058\n",
      "Epoch: 6 \t Training loss: 0.263793468475\n",
      "Epoch: 6 \t Training loss: 0.293715059757\n",
      "Epoch: 6 \t Training loss: 0.270985305309\n",
      "Epoch: 6 \t Training loss: 0.268092244864\n",
      "Epoch: 6 \t Training loss: 0.276248544455\n",
      "Epoch: 6 \t Training loss: 0.255617439747\n",
      "Epoch: 6 \t Training loss: 0.285190761089\n",
      "Epoch: 6 \t Training loss: 0.270159721375\n",
      "Epoch: 6 \t Training loss: 0.27812898159\n",
      "Epoch: 6 \t Training loss: 0.276609510183\n",
      "Epoch: 6 \t Training loss: 0.270101636648\n",
      "Epoch: 6 \t Training loss: 0.259791582823\n",
      "Epoch: 6 \t Training loss: 0.293550938368\n",
      "Epoch: 6 \t Training loss: 0.27202001214\n",
      "Epoch: 6 \t Training loss: 0.236667603254\n",
      "Epoch: 6 \t Validation Accuracy: 0.882920086384\n",
      "Epoch: 7 \t Training loss: 0.276035189629\n",
      "Epoch: 7 \t Training loss: 0.271819978952\n",
      "Epoch: 7 \t Training loss: 0.272101223469\n",
      "Epoch: 7 \t Training loss: 0.280148118734\n",
      "Epoch: 7 \t Training loss: 0.259714961052\n",
      "Epoch: 7 \t Training loss: 0.263982117176\n",
      "Epoch: 7 \t Training loss: 0.275444298983\n",
      "Epoch: 7 \t Training loss: 0.264421701431\n",
      "Epoch: 7 \t Training loss: 0.258025795221\n",
      "Epoch: 7 \t Training loss: 0.262917608023\n",
      "Epoch: 7 \t Training loss: 0.257946819067\n",
      "Epoch: 7 \t Training loss: 0.286170899868\n",
      "Epoch: 7 \t Training loss: 0.265099644661\n",
      "Epoch: 7 \t Training loss: 0.261663764715\n",
      "Epoch: 7 \t Training loss: 0.269656687975\n",
      "Epoch: 7 \t Training loss: 0.249248698354\n",
      "Epoch: 7 \t Training loss: 0.277245789766\n",
      "Epoch: 7 \t Training loss: 0.264004707336\n",
      "Epoch: 7 \t Training loss: 0.2716139853\n",
      "Epoch: 7 \t Training loss: 0.269246250391\n",
      "Epoch: 7 \t Training loss: 0.26303628087\n",
      "Epoch: 7 \t Training loss: 0.252829432487\n",
      "Epoch: 7 \t Training loss: 0.285791486502\n",
      "Epoch: 7 \t Training loss: 0.264395475388\n",
      "Epoch: 7 \t Training loss: 0.230621889234\n",
      "Epoch: 7 \t Validation Accuracy: 0.882960617542\n",
      "Epoch: 8 \t Training loss: 0.270253896713\n",
      "Epoch: 8 \t Training loss: 0.265853285789\n",
      "Epoch: 8 \t Training loss: 0.266145765781\n",
      "Epoch: 8 \t Training loss: 0.273832917213\n",
      "Epoch: 8 \t Training loss: 0.253333508968\n",
      "Epoch: 8 \t Training loss: 0.258199900389\n",
      "Epoch: 8 \t Training loss: 0.268798679113\n",
      "Epoch: 8 \t Training loss: 0.257785022259\n",
      "Epoch: 8 \t Training loss: 0.251353889704\n",
      "Epoch: 8 \t Training loss: 0.256824821234\n",
      "Epoch: 8 \t Training loss: 0.252184212208\n",
      "Epoch: 8 \t Training loss: 0.279384553432\n",
      "Epoch: 8 \t Training loss: 0.259298682213\n",
      "Epoch: 8 \t Training loss: 0.255718737841\n",
      "Epoch: 8 \t Training loss: 0.263687580824\n",
      "Epoch: 8 \t Training loss: 0.243755087256\n",
      "Epoch: 8 \t Training loss: 0.270652413368\n",
      "Epoch: 8 \t Training loss: 0.258590340614\n",
      "Epoch: 8 \t Training loss: 0.266031146049\n",
      "Epoch: 8 \t Training loss: 0.26353982091\n",
      "Epoch: 8 \t Training loss: 0.257489860058\n",
      "Epoch: 8 \t Training loss: 0.247187942266\n",
      "Epoch: 8 \t Training loss: 0.278262495995\n",
      "Epoch: 8 \t Training loss: 0.257577955723\n",
      "Epoch: 8 \t Training loss: 0.224553629756\n",
      "Epoch: 8 \t Validation Accuracy: 0.882717549801\n",
      "Epoch: 9 \t Training loss: 0.263579010963\n",
      "Epoch: 9 \t Training loss: 0.258933484554\n",
      "Epoch: 9 \t Training loss: 0.259452730417\n",
      "Epoch: 9 \t Training loss: 0.266683608294\n",
      "Epoch: 9 \t Training loss: 0.246513918042\n",
      "Epoch: 9 \t Training loss: 0.251444995403\n",
      "Epoch: 9 \t Training loss: 0.262038558722\n",
      "Epoch: 9 \t Training loss: 0.250901579857\n",
      "Epoch: 9 \t Training loss: 0.242658644915\n",
      "Epoch: 9 \t Training loss: 0.249411255121\n",
      "Epoch: 9 \t Training loss: 0.244100376964\n",
      "Epoch: 9 \t Training loss: 0.270867228508\n",
      "Epoch: 9 \t Training loss: 0.252684980631\n",
      "Epoch: 9 \t Training loss: 0.247909918427\n",
      "Epoch: 9 \t Training loss: 0.254592567682\n",
      "Epoch: 9 \t Training loss: 0.23481811583\n",
      "Epoch: 9 \t Training loss: 0.25790438056\n",
      "Epoch: 9 \t Training loss: 0.248528033495\n",
      "Epoch: 9 \t Training loss: 0.256322592497\n",
      "Epoch: 9 \t Training loss: 0.255121409893\n",
      "Epoch: 9 \t Training loss: 0.246949583292\n",
      "Epoch: 9 \t Training loss: 0.237165510654\n",
      "Epoch: 9 \t Training loss: 0.26546049118\n",
      "Epoch: 9 \t Training loss: 0.246712163091\n",
      "Epoch: 9 \t Training loss: 0.215801641345\n",
      "Epoch: 9 \t Validation Accuracy: 0.880124747753\n",
      "Test Accuracy: 0.880691945553\n",
      "Time to completion: 1026164.38103ms\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "batch_size = 3000\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "# Initializing the variables\n",
    "tic = time.time()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for epoch in range(epochs):\n",
    "    for X_batch, y_batch in get_batch(onehot_enc.transform(X_train), label2bool(y_train), batch_size):         \n",
    "        loss_value, _ = sess.run([loss, optimizer], feed_dict={\n",
    "            inputs_: X_batch,\n",
    "            targets_: y_batch\n",
    "        })\n",
    "        print(\"Epoch: {} \\t Training loss: {}\".format(epoch, loss_value))\n",
    "\n",
    "    acc = sess.run(accuracy, feed_dict={\n",
    "        inputs_: onehot_enc.transform(X_valid),\n",
    "        targets_: label2bool(y_valid)\n",
    "    })\n",
    "\n",
    "    print(\"Epoch: {} \\t Validation Accuracy: {}\".format(epoch, acc))\n",
    "\n",
    "test_acc = sess.run(accuracy, feed_dict={\n",
    "    inputs_: onehot_enc.transform(X_test),\n",
    "    targets_: label2bool(y_test)\n",
    "})\n",
    "print(\"Test Accuracy: {}\".format(test_acc))\n",
    "toc = time.time()\n",
    "print(\"Time to completion: \" + str(1000*(toc-tic)) + \"ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
